{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "from gensim.utils import simple_preprocess\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from lbl2vec import Lbl2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from urllib.parse import quote_plus\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "KEY_311 = os.environ.get('311_API_KEY')\n",
    "AUTH_311 = HTTPBasicAuth('key', KEY_311)\n",
    "API_URL_311 = 'https://boston2-production.spotmobile.net/open311/v2/services.json'\n",
    "ILLEGAL_PARKING_SERVICE_CODE = quote_plus(\"Transportation - Traffic Division:Enforcement & Abandoned Vehicles:Parking Enforcement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline:\n",
    "0. Data Collection\n",
    "1. Data Preparation\n",
    "3. Data Preprocessing\n",
    "4. Model Training\n",
    "5. Model Evaluation\n",
    "6. Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to gather service request JSON objects from the Boston Open311 API Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods will request all available service requests (limited to last 90 days) and write the resulting JSON array of service requests to the given filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_single_query(page_num:int):\n",
    "    ''' \n",
    "    Sends an HTTP GET request to the Boston 311 API for Illegal Parking.\n",
    "    Requests 100 (maximum allowed) service requests for a given page.\n",
    "    \n",
    "    Returns: HTTP Response Object\n",
    "    '''\n",
    "    query_url = f'https://boston2-production.spotmobile.net/open311/v2/requests.json?service_code={ILLEGAL_PARKING_SERVICE_CODE}&per_page=100&page={page_num}'\n",
    "\n",
    "    return requests.get(url=query_url, auth=AUTH_311)\n",
    "\n",
    "def retreive_all_service_requests(filename:str):\n",
    "    ''' \n",
    "    Continually sends HTTP Requests to the Boston 311 API for Illegal Parking Service Requests.\n",
    "    Concatenates responses into a single JSON Array containing Service Request JSON Objects.\n",
    "    Updates are sent to the console throughout the process.\n",
    "    Returns: writes the JSON Array to the given filename, once all service requests are received.\n",
    "    '''\n",
    "\n",
    "    # Executes first query\n",
    "    page = 1\n",
    "    current_response = execute_single_query(page)\n",
    "\n",
    "    # Ensure HTTP Error is captured\n",
    "    if current_response.status_code != 200:\n",
    "        print(f'First Query failed. Error code: {current_response.status_code}')\n",
    "        return\n",
    "    page+=1\n",
    "    \n",
    "    # Creates JSON Array Object on response\n",
    "    all_service_requests_json = current_response.json()\n",
    "\n",
    "    # Continually execute queries for the next page until there are no more (Error Code != 200, 429)\n",
    "    while True:\n",
    "        current_response = execute_single_query(page)\n",
    "        status_code = current_response.status_code\n",
    "\n",
    "        # Successful response has service request JSON Objects concatenated to end of overall JSON Array\n",
    "        if status_code == 200:\n",
    "            print(f'Query Received: {page}')\n",
    "            for service_request_obj in current_response.json():\n",
    "                all_service_requests_json.append(service_request_obj)\n",
    "            page += 1\n",
    "        \n",
    "        # Error code 429 indicates rate limiting (10 GET Requests per min)\n",
    "        elif status_code == 429:\n",
    "            print(f'Rate Limited: Waiting 1 minute. \\n Current Page {page}')\n",
    "            # Wait 60 seconds to resume query\n",
    "            time.sleep(60)\n",
    "            print('Resuming Query...')\n",
    "        \n",
    "        # Any other error code, break loop.\n",
    "        else: \n",
    "            print(f'Error recieved: {status_code}')\n",
    "            break\n",
    "\n",
    "    print(f'Finished querying. Total pages queried: {page}\\n Printing...')\n",
    "\n",
    "    # Write received service requests to specificed filename\n",
    "    all_service_requests_file = open(filename, 'w')\n",
    "    all_service_requests_file.write(json.dumps(all_service_requests_json))\n",
    "    all_service_requests_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_REQ_JSON_FILE_NAME = 'service_requests_last_90_days.json'\n",
    "retreive_all_service_requests(SERVICE_REQ_JSON_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method prepares the JSON array of service request JSON objects, extracting the free-form text description and service request ID from each object. These description and ID pairs are stored in a dictionary which is then pickled for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_request_id_with_description(filename:str) -> dict:\n",
    "    ''' \n",
    "    For a given file containing a JSON Array of Service Request JSON Objects, extract the service number and the description\n",
    "    (if it exists) into a dictionary for further preprocessing.\n",
    "\n",
    "    Returns: Dict (Service Request ID -> Service Request Description)\n",
    "    '''\n",
    "    request_id_descr_dict = {}\n",
    "    service_requests_json = json.loads(open(filename, 'r').read())\n",
    "\n",
    "    for service_request in service_requests_json:\n",
    "        # Ensure the description exists for this service request\n",
    "        description = service_request.get('description', False)\n",
    "        if not description: continue   \n",
    "\n",
    "        # Add to dictionary using service request ID as key and service request description as value.\n",
    "        request_id_descr_dict[service_request['service_request_id']] = service_request['description']\n",
    "\n",
    "    return request_id_descr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLED_SERVICE_REQ_DESC = 'service_req_last_90_days.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and store it as a python pickle file for later use.    \n",
    "service_requests_with_description_file = open(PICKLED_SERVICE_REQ_DESC, 'wb')\n",
    "\n",
    "dict = extract_request_id_with_description(SERVICE_REQ_JSON_FILE_NAME)\n",
    "\n",
    "pickle.dump(dict, service_requests_with_description_file)\n",
    "\n",
    "service_requests_with_description_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple_preprocess method from gensim removes accents and special characters, then downcasing and tokenizing the document, removing any word with length less than 2 or greater than 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters and downcase, then tokenize\n",
    "def tokenize(doc):\n",
    "    return simple_preprocess(doc, deacc=True, min_len=2, max_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Testing Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After retreiving the pickled service request ID to description dictionary, create a DataFrame to store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled service descriptions\n",
    "with open(PICKLED_SERVICE_REQ_DESC, 'rb') as file:\n",
    "    service_id_to_descriptions:dict = pickle.load(file)\n",
    "\n",
    "service_drescriptions_df = pd.DataFrame(data=service_id_to_descriptions.items(), columns=['ID', 'Description'])\n",
    "service_drescriptions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data between training, validation, and testing using sklearn's train_test_split() method. The data is split 70/15/15 between training, validation, and testing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train=.70, Validate=.15, Test=.15\n",
    "# Split all data between train, (test AND validate)\n",
    "desc_train, desc_testval = train_test_split(service_drescriptions_df, test_size=.3)\n",
    "\n",
    "# Split (test AND validate) between test, validate\n",
    "# .3 * .5 = 0.15\n",
    "desc_val, desc_test = train_test_split(desc_testval, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the data, mark its data type and re-concatenate into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_train['data_type'] = 'train'\n",
    "desc_test['data_type'] = 'test'\n",
    "desc_val['data_type'] = 'validate'\n",
    "\n",
    "all_descriptions = pd.concat([desc_train, desc_test, desc_val]).reset_index(drop=True)\n",
    "all_descriptions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lbl2Vec model takes in tagged documents, where each document has an integer tag which is used to keep track of the document, its vector, and labels. In order to use these labels after the model is trained, a function is created which maps the service request ID to a document tag integer. The service request ID cannot be used as a document tag because when the Lbl2Vec model sees a tag like 101005244674, it assumes there are documents with tags from [0, 101005244674], resulting in too much memory being allocated. Currently, the function simply uses the index of a list of service request IDs as the document tags, which is not a very sustainable hash function at scale, but it does ensure the document tags are as small as possible, avoiding the memory allocation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_id_list = []\n",
    "def service_id_to_tag(service_id:int) -> int:\n",
    "    ''' \n",
    "    Simple hash function for converting Service Request IDs to document tags.\n",
    "    Simply uses the index in a list of Service Request IDs as the document tag.\n",
    "    This is not the most efficient implementation, but it minimizes the size of the document tag integers.\n",
    "    '''\n",
    "    if service_id in service_id_list:\n",
    "        return service_id_list.index(service_id)\n",
    "    service_id_list.append(service_id)\n",
    "    return len(service_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each description in the DataFrame, apply tokenization and tag the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descriptions['tagged_desc'] = all_descriptions.apply(lambda row: TaggedDocument(tokenize(row['Description']), [service_id_to_tag(int(row['ID']))]), axis=1)\n",
    "all_descriptions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels:\n",
    "\n",
    "    1.  Bike Lane Obstruction: bike, cycle, path,  \n",
    "    2.  Bus Lane Obstruction: bus, stop, \n",
    "    3.  Non-resident Parking: resident, state\n",
    "    4.  Blocked Fire Hydrant: fire, hydrant\n",
    "    5.  Blocked Sidewalk: sidewalk, side\n",
    "    6.  Blocked Driveway: driveway, drive, way, private\n",
    "    7.  Blocked Crosswalk: crosswalk, cross\n",
    "    8.  Blocked Handicap spot: handicap, placard,\n",
    "    9.  Double Parking: double, triple\n",
    "    10. No stopping zone: stopping, zone\n",
    "    11. visitor spot: visitor, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_keywords_dict = {\n",
    "    'bike lane' :           ['bike', 'cycle', 'cycling', 'path', 'sharrow'],\n",
    "    'bus lane' :            ['bus'],\n",
    "    'resident parking' :    ['resident', 'state', 'plate', 'plates'],\n",
    "    'fire hydrant' :        ['fire', 'hydrant'],\n",
    "    'sidewalk' :            ['sidewalk', 'side'],\n",
    "    'driveway' :            ['driveway', 'drive', 'way', 'private'],\n",
    "    'crosswalk' :           ['crosswalk', 'cross'],\n",
    "    'handicap' :            ['handicap', 'placard'],\n",
    "    'double parking' :      ['double', 'triple'],\n",
    "    'no stopping' :         ['stopping', 'loading'],\n",
    "    'visitor parking' :     ['visitor', 'hour']\n",
    "} \n",
    "\n",
    "NUM_LABELS = len(category_keywords_dict.keys())\n",
    "\n",
    "category_keywords = pd.DataFrame(data=category_keywords_dict.items(), columns=['category', 'keywords'])\n",
    "category_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through hyperparameter tuning, the optimal hyperparameters for the Lbl2Vec model are as follows:\n",
    "   \n",
    "    min_count=1\n",
    "    window=10\n",
    "    similarity_threshold=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lbl2Vec(\n",
    "    keywords_list=list(category_keywords['keywords']),\n",
    "    tagged_documents=all_descriptions['tagged_desc'][all_descriptions['data_type'] == 'train'],\n",
    "    min_count=1, \n",
    "    similarity_threshold=0.3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling training documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_model_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run label prediction on the testing dataset to generate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_documents = all_descriptions['tagged_desc'][all_descriptions.data_type == 'testing']\n",
    "\n",
    "testing_label_predictions = model.predict_new_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA reduces the dimensionality for the vectors from 300 to 2, allowing for plotting and visual inspection of clustering along with analytical evaluation like Silhouette scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `simScores` is your DataFrame\n",
    "features = [f'label_{i}' for i in range(NUM_LABELS)]  # Adjust range based on your labels\n",
    "X = testing_label_predictions[features]\n",
    "\n",
    "# Apply PCA to reduce dimensions\n",
    "pca = PCA(n_components=2)  # Adjust `n_components` as needed\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering with K-means\n",
    "kmeans = KMeans(n_clusters=NUM_LABELS, random_state=42)\n",
    "kmeans.fit(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate silhouette score\n",
    "silhouette_avg = silhouette_score(X_pca, kmeans.labels_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the vector clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the first two PCA components\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis', marker='o', edgecolor='k', s=50, alpha=0.6)\n",
    "\n",
    "# Plot centroids\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.9, marker='X')\n",
    "\n",
    "plt.title('Clusters of Documents')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
